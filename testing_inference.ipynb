{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyCBC.libutils: pkg-config call failed, setting NO_PKGCONFIG=1\n"
     ]
    }
   ],
   "source": [
    "from populations.bbh_models import get_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_models('../OneChannel_Flows/models_reduced.hdf5',['CE'],['mchirp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "from scipy.special import logit\n",
    "from scipy.special import expit\n",
    "\n",
    "def logistic(data,rescaling=False, wholedataset=True, max =1, rescale_max=1):\n",
    "    if rescaling:\n",
    "        if wholedataset:\n",
    "            rescale_max = np.max(data) + 0.01\n",
    "        else:\n",
    "            rescale_max = rescale_max\n",
    "        data /= rescale_max\n",
    "    else:\n",
    "        rescale_max = None\n",
    "    data = logit(data)\n",
    "    if wholedataset:\n",
    "        max = np.max(data)\n",
    "    else:\n",
    "        max = max\n",
    "    data /= max\n",
    "    return([data, max, rescale_max])\n",
    "\n",
    "def expistic(data, max, rescale_max=None):\n",
    "    data*=max\n",
    "    data = expit(data)\n",
    "    if rescale_max != None:\n",
    "        data *=rescale_max\n",
    "    return(data)\n",
    "\n",
    "def get_model_keys(path):\n",
    "    alpha_val = '10'\n",
    "    all_models = []\n",
    "    models = []\n",
    "    def find_submodels(name, obj):\n",
    "        if isinstance(obj, h5py.Dataset):\n",
    "            all_models.append(name.rsplit('/', 1)[0])\n",
    "            \n",
    "    f = h5py.File(path, 'r')\n",
    "    f.visititems(find_submodels)\n",
    "    # get all unique models\n",
    "    all_models = sorted(list(set(all_models)))\n",
    "    f.close()\n",
    "\n",
    "    # use only models with given alpha value\n",
    "    for model in all_models:\n",
    "        if 'alpha' in model:\n",
    "            if 'alpha'+alpha_val in model:\n",
    "                models.append('/'+model)\n",
    "        else:\n",
    "            models.append('/' + model)\n",
    "    return(np.split(np.array(models), 5))\n",
    "\n",
    "def get_model_keys_CE(path):\n",
    "    all_models = []\n",
    "    models = []\n",
    "    def find_submodels(name, obj):\n",
    "        if isinstance(obj, h5py.Dataset):\n",
    "            all_models.append(name.rsplit('/', 1)[0])\n",
    "            \n",
    "    f = h5py.File(path, 'r')\n",
    "    f.visititems(find_submodels)\n",
    "    # get all unique models\n",
    "    all_models = sorted(list(set(all_models)))\n",
    "    f.close()\n",
    "\n",
    "    # use only models with given alpha value\n",
    "    for model in all_models:\n",
    "        if 'CE' in model:\n",
    "            models.append('/'+model)\n",
    "    return(np.split(np.array(models), 4))\n",
    "\n",
    "def read_hdf5(path, all_alpha=False):\n",
    "    if all_alpha:\n",
    "        popsynth_outputs = {}\n",
    "        models = np.asarray(get_model_keys_CE(path))\n",
    "        for i in range(models.shape[0]):\n",
    "            for j in range(models.shape[1]):\n",
    "                popsynth_outputs[i,j]=pd.read_hdf(path, key=models[i,j])\n",
    "    else:\n",
    "        popsynth_outputs = {}\n",
    "        models = np.asarray(get_model_keys(path))\n",
    "        for i in range(models.shape[0]):\n",
    "            for j in range(models.shape[1]):\n",
    "                popsynth_outputs[i,j]=pd.read_hdf(path, key=models[i,j])\n",
    "\n",
    "    return(popsynth_outputs)\n",
    "\n",
    "def plot_histogram(model_id, param, axes, models_path):\n",
    "    popsynth_outputs = read_hdf5(models_path)\n",
    "    axes.hist(popsynth_outputs[model_id][param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path ='/Users/stormcolloms/Documents/PhD/Project_work/OneChannel_Flows/models_reduced.hdf5'\n",
    "samples = read_hdf5(models_path, all_alpha=False)\n",
    "\n",
    "cond_inputs = 1\n",
    "\n",
    "channel_label = 'SMT'\n",
    "params = ['mchirp','q', 'chieff', 'z']\n",
    "no_params = 4\n",
    "\n",
    "chi_b = [0.,0.1,0.2,0.5]\n",
    "alpha = [0.2,0.5,1.,2.,5.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000000.       0.       0.       0.] [1000000.       0.       0.       0.] 4 4000000\n",
      "[1000000. 1000000.       0.       0.] [1000000. 2000000.       0.       0.] 4 4000000\n",
      "[1000000. 1000000. 1000000.       0.] [1000000. 2000000. 3000000.       0.] 4 4000000\n",
      "[1000000. 1000000. 1000000. 1000000.] [1000000. 2000000. 3000000. 4000000.] 4 4000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "channel_ids = {'CE':0, 'CHE':1,'GC':2,'NSC':3, 'SMT':4}\n",
    "channel_id = channel_ids[channel_label]\n",
    "#number of data points (total) for each channel\n",
    "channel_samples = [1e6,864124,896611,582961, 4e6]\n",
    "no_binaries = int(channel_samples[channel_id])\n",
    "\n",
    "params = params + ['weight'] #read in weights as well\n",
    "\n",
    "if cond_inputs == 1:\n",
    "    #Channels with 1D hyperparameters: SMT, GC, NSC, CHE\n",
    "\n",
    "    #put data from required parameters for all alphas and chi_bs into model_stack\n",
    "    models = np.zeros((no_binaries,no_params+1))\n",
    "    model_size = np.zeros(no_params)\n",
    "    cumulsize = np.zeros(no_params)\n",
    "\n",
    "    #stack data\n",
    "    for chib_id, xb in enumerate(chi_b):\n",
    "        model_size[chib_id] = np.shape(samples[(channel_id,chib_id)][params])[0]\n",
    "        cumulsize[chib_id] = np.sum(model_size)\n",
    "        models[int(cumulsize[chib_id-1]):int(np.sum(model_size))]=np.asarray(samples[(channel_id,chib_id)][params])\n",
    "\n",
    "        models_stack = np.copy(models) #np.concatenate(models, axis=0)\n",
    "\n",
    "    #logit and renormalise distributions pre-batching\n",
    "    models_stack[:,0], max_logit_mchirp, max_mchirp = logistic(models_stack[:,0], True)\n",
    "    if channel_id == 2: #add extra tiny amount to GC mass ratios as q=1 samples exist\n",
    "        models_stack[:,1], max_q, extra_scale = logistic(models_stack[:,1], True)\n",
    "    else:\n",
    "        models_stack[:,1], max_q, _ = logistic(models_stack[:,1])\n",
    "    models_stack[:,2] = np.arctanh(models_stack[:,2])\n",
    "    models_stack[:,3],max_logit_z, max_z = logistic(models_stack[:,3], True)\n",
    "\n",
    "    training_hps_stack = np.repeat(chi_b, (model_size).astype(int), axis=0)\n",
    "    training_hps_stack = np.reshape(training_hps_stack,(-1,1))\n",
    "    validation_hps_stack = np.reshape(training_hps_stack,(-1,1))\n",
    "    train_models_stack = models_stack\n",
    "    validation_models_stack = models_stack\n",
    "\n",
    "else:\n",
    "    #CE channel with alpha parameter treatment\n",
    "\n",
    "    #put data from required parameters for all alphas and chi_bs into model_stack\n",
    "    models = np.zeros((4,5,no_binaries,no_params+1))\n",
    "    removed_model_id =[7,11]\n",
    "    val_hps = [[0.1,1],[0.2,.5]]\n",
    "\n",
    "    #format which chi_bs and alphas match which parameter values being read in\n",
    "    chi_b_alpha_pairs= np.zeros((20,2))\n",
    "    chi_b_alpha_pairs[:,0] = np.repeat(chi_b,np.shape(alpha)[0])\n",
    "    chi_b_alpha_pairs[:,1] = np.tile(alpha, np.shape(chi_b)[0])\n",
    "\n",
    "    training_hp_pairs = np.delete(chi_b_alpha_pairs, removed_model_id, 0) #removes [0.1,1] and [0.2,0.5] point\n",
    "    training_hps_stack = np.repeat(training_hp_pairs, no_binaries, axis=0) #repeats to cover all samples in each population\n",
    "    validation_hps_stack = np.repeat(val_hps, no_binaries, axis=0)\n",
    "    all_chi_b_alphas = np.repeat(chi_b_alpha_pairs, no_binaries, axis=0)\n",
    "\n",
    "    #stack data\n",
    "    for chib_id in range(4):\n",
    "        for alpha_id in range(5):\n",
    "            models[chib_id, alpha_id]=np.asarray(samples[(chib_id, alpha_id)][params])\n",
    "\n",
    "    #removing the sepeartion of chi_b and alpha into axes and just stringing them all together instead\n",
    "    joined_chib_samples = np.concatenate(models, axis=0)\n",
    "    models_stack = np.concatenate(joined_chib_samples, axis=0) #all models if needed\n",
    "\n",
    "    #logit and renormalise distributions pre-batching\n",
    "    #chirp mass original range 0 to inf\n",
    "    joined_chib_samples[:,:,0], max_logit_mchirp, max_mchirp = logistic(joined_chib_samples[:,:,0], True)\n",
    "\n",
    "    #mass ratio - original range 0 to 1\n",
    "    joined_chib_samples[:,:,1], max_q, _ = logistic(joined_chib_samples[:,:,1])\n",
    "\n",
    "    #chieff - original range -0.5 to +1\n",
    "    joined_chib_samples[:,:,2] = np.arctanh(joined_chib_samples[:,:,2])\n",
    "\n",
    "    #redshift - original range 0 to inf\n",
    "    joined_chib_samples[:,:,3], max_logit_z, max_z = logistic(joined_chib_samples[:,:,3], True)\n",
    "\n",
    "    #keep samples seperated by model id (combined chi_b and alpha id) until validation samples are removed, then concatenate\n",
    "    train_models = np.delete(joined_chib_samples, removed_model_id, 0) #removes samples from validation models\n",
    "    train_models_stack = np.concatenate(train_models, axis=0)\n",
    "\n",
    "    validation_model = joined_chib_samples[removed_model_id,:,:]\n",
    "    validation_models_stack = np.concatenate(validation_model, axis=0)\n",
    "\n",
    "#concatenate data plus weights with hyperparams\n",
    "training_data = np.concatenate((train_models_stack, training_hps_stack), axis=1)\n",
    "val_data = np.concatenate((validation_models_stack, validation_hps_stack), axis=1)\n",
    "mappings = np.asarray([max_logit_mchirp, max_mchirp, max_q, None, max_logit_z, max_z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(896611, 5)\n",
      "(896611, 1)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(train_models_stack))\n",
    "print(np.shape(training_hps_stack))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('amaze')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40f4eedd3c2b0a4cf443053300595e4c410b663f293e84798d1742802fb2c7c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
