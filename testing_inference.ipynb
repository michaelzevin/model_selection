{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from populations.bbh_models import get_models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]/Users/stormcolloms/opt/anaconda3/envs/amaze/lib/python3.9/site-packages/torch/nn/init.py:405: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.37s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['CE/chi00/alpha02',\n",
       "  'CE/chi00/alpha05',\n",
       "  'CE/chi00/alpha10',\n",
       "  'CE/chi00/alpha20',\n",
       "  'CE/chi00/alpha50',\n",
       "  'CE/chi01/alpha02',\n",
       "  'CE/chi01/alpha05',\n",
       "  'CE/chi01/alpha10',\n",
       "  'CE/chi01/alpha20',\n",
       "  'CE/chi01/alpha50',\n",
       "  'CE/chi02/alpha02',\n",
       "  'CE/chi02/alpha05',\n",
       "  'CE/chi02/alpha10',\n",
       "  'CE/chi02/alpha20',\n",
       "  'CE/chi02/alpha50',\n",
       "  'CE/chi05/alpha02',\n",
       "  'CE/chi05/alpha05',\n",
       "  'CE/chi05/alpha10',\n",
       "  'CE/chi05/alpha20',\n",
       "  'CE/chi05/alpha50'],\n",
       " {'CE': <populations.Flowsclass_dev.FlowModel at 0x7fdb42150eb0>})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_models('../OneChannel_Flows/models_reduced.hdf5',['CE'],['mchirp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "from scipy.special import logit\n",
    "from scipy.special import expit\n",
    "\n",
    "def logistic(data,rescaling=False, wholedataset=True, max =1, rescale_max=1):\n",
    "    if rescaling:\n",
    "        if wholedataset:\n",
    "            rescale_max = np.max(data) + 0.01\n",
    "        else:\n",
    "            rescale_max = rescale_max\n",
    "        data /= rescale_max\n",
    "    else:\n",
    "        rescale_max = None\n",
    "    data = logit(data)\n",
    "    if wholedataset:\n",
    "        max = np.max(data)\n",
    "    else:\n",
    "        max = max\n",
    "    data /= max\n",
    "    return([data, max, rescale_max])\n",
    "\n",
    "def expistic(data, max, rescale_max=None):\n",
    "    data*=max\n",
    "    data = expit(data)\n",
    "    if rescale_max != None:\n",
    "        data *=rescale_max\n",
    "    return(data)\n",
    "\n",
    "def get_model_keys(path):\n",
    "    alpha_val = '10'\n",
    "    all_models = []\n",
    "    models = []\n",
    "    def find_submodels(name, obj):\n",
    "        if isinstance(obj, h5py.Dataset):\n",
    "            all_models.append(name.rsplit('/', 1)[0])\n",
    "            \n",
    "    f = h5py.File(path, 'r')\n",
    "    f.visititems(find_submodels)\n",
    "    # get all unique models\n",
    "    all_models = sorted(list(set(all_models)))\n",
    "    f.close()\n",
    "\n",
    "    # use only models with given alpha value\n",
    "    for model in all_models:\n",
    "        if 'alpha' in model:\n",
    "            if 'alpha'+alpha_val in model:\n",
    "                models.append('/'+model)\n",
    "        else:\n",
    "            models.append('/' + model)\n",
    "    return(np.split(np.array(models), 5))\n",
    "\n",
    "def get_model_keys_CE(path):\n",
    "    all_models = []\n",
    "    models = []\n",
    "    def find_submodels(name, obj):\n",
    "        if isinstance(obj, h5py.Dataset):\n",
    "            all_models.append(name.rsplit('/', 1)[0])\n",
    "            \n",
    "    f = h5py.File(path, 'r')\n",
    "    f.visititems(find_submodels)\n",
    "    # get all unique models\n",
    "    all_models = sorted(list(set(all_models)))\n",
    "    f.close()\n",
    "\n",
    "    # use only models with given alpha value\n",
    "    for model in all_models:\n",
    "        if 'CE' in model:\n",
    "            models.append('/'+model)\n",
    "    return(np.split(np.array(models), 4))\n",
    "\n",
    "def read_hdf5(path, all_alpha=False):\n",
    "    if all_alpha:\n",
    "        popsynth_outputs = {}\n",
    "        models = np.asarray(get_model_keys_CE(path))\n",
    "        for i in range(models.shape[0]):\n",
    "            for j in range(models.shape[1]):\n",
    "                popsynth_outputs[i,j]=pd.read_hdf(path, key=models[i,j])\n",
    "    else:\n",
    "        popsynth_outputs = {}\n",
    "        models = np.asarray(get_model_keys(path))\n",
    "        for i in range(models.shape[0]):\n",
    "            for j in range(models.shape[1]):\n",
    "                popsynth_outputs[i,j]=pd.read_hdf(path, key=models[i,j])\n",
    "\n",
    "    return(popsynth_outputs)\n",
    "\n",
    "def plot_histogram(model_id, param, axes, models_path):\n",
    "    popsynth_outputs = read_hdf5(models_path)\n",
    "    axes.hist(popsynth_outputs[model_id][param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path ='/Users/stormcolloms/Documents/PhD/Project_work/OneChannel_Flows/models_reduced.hdf5'\n",
    "samples = read_hdf5(models_path, all_alpha=False)\n",
    "\n",
    "cond_inputs = 1\n",
    "\n",
    "channel_label = 'SMT'\n",
    "params = ['mchirp','q', 'chieff', 'z']\n",
    "no_params = 4\n",
    "\n",
    "chi_b = [0.,0.1,0.2,0.5]\n",
    "alpha = [0.2,0.5,1.,2.,5.]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test reading in flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import populations.bbh_models as read_models\n",
    "from populations.utils.flow import NFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.74s/it]\n"
     ]
    }
   ],
   "source": [
    "channels = ['SMT']\n",
    "file_path='/Users/stormcolloms/Documents/PhD/Project_work/OneChannel_Flows/models_reduced.hdf5'\n",
    "params =['mchirp', 'q','chieff', 'z']\n",
    "\n",
    "flow = read_models.get_models(file_path, channels, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m((\u001b[39mlist\u001b[39m(flow\u001b[39m.\u001b[39;49mkeys())))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "print((list(flow.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialise flow first\n",
      "(4000000, 6)\n",
      "4000000\n",
      "(4000000, 6)\n",
      "4000000\n",
      "(4000000, 6)\n",
      "4000000\n",
      "(4000000, 6)\n",
      "4000000\n",
      "(4000000, 6)\n",
      "4000000\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 11975967 is out of bounds for axis 0 with size 4000000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m flow_model\u001b[39m=\u001b[39m flow[\u001b[39m'\u001b[39m\u001b[39mSMT\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m flow_model\u001b[39m.\u001b[39;49mtrain(lr\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, batch_no\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, filename\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtest_training_SMT.pt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/PhD/Project_work/AMAZE_model_selection/populations/Flowsclass_dev.py:385\u001b[0m, in \u001b[0;36mFlowModel.train\u001b[0;34m(self, lr, epochs, batch_no, filename)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m, lr, epochs, batch_no, filename):\n\u001b[1;32m    384\u001b[0m     training_data, val_data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmappings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmap_samples(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams)\n\u001b[0;32m--> 385\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflow\u001b[39m.\u001b[39;49mtrainval(lr, epochs, batch_no, filename, training_data, val_data)\n",
      "File \u001b[0;32m~/Documents/PhD/Project_work/AMAZE_model_selection/populations/utils/flow.py:130\u001b[0m, in \u001b[0;36mNFlow.trainval\u001b[0;34m(self, lr, epochs, batch_no, filename, training_data, val_data)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m# Validate\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad(): \u001b[39m#disables gradient caluclation\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     \u001b[39m#call validation data\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     x_val, x_conditional_val, x_weights_val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_val_data(val_data)\n\u001b[1;32m    131\u001b[0m     val_loss_g\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m    134\u001b[0m     \u001b[39m#evaluate flow parameters\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/PhD/Project_work/AMAZE_model_selection/populations/utils/flow.py:420\u001b[0m, in \u001b[0;36mNFlow.get_val_data\u001b[0;34m(self, validation_data)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    419\u001b[0m     random_samples \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mno_params\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mno_binaries,size\u001b[39m=\u001b[39m(\u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size\u001b[39m*\u001b[39m\u001b[39m0.2\u001b[39m)))\n\u001b[0;32m--> 420\u001b[0m     validation_hp_pairs \u001b[39m=\u001b[39m validation_data[random_samples,\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\n\u001b[1;32m    422\u001b[0m validation_samples \u001b[39m=\u001b[39m validation_data[random_samples,:(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mno_params)]\n\u001b[1;32m    423\u001b[0m val_weights \u001b[39m=\u001b[39m validation_data[random_samples,\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: index 11975967 is out of bounds for axis 0 with size 4000000"
     ]
    }
   ],
   "source": [
    "flow_model= flow['SMT']\n",
    "flow_model.train(lr=0.001, epochs=5, batch_no=5, filename='test_training_SMT.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.load_model('test_training.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test Sampling 28/6/23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:11<00:00,  2.21s/it]\n"
     ]
    }
   ],
   "source": [
    "channels = ['CE','CHE','GC','NSC','SMT']\n",
    "model_names, kde_models=read_models.get_models(file_path, channels, params, useKDE=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CE/chi00/alpha02', 'CE/chi00/alpha05', 'CE/chi00/alpha10', 'CE/chi00/alpha20', 'CE/chi00/alpha50', 'CE/chi01/alpha02', 'CE/chi01/alpha05', 'CE/chi01/alpha10', 'CE/chi01/alpha20', 'CE/chi01/alpha50', 'CE/chi02/alpha02', 'CE/chi02/alpha05', 'CE/chi02/alpha10', 'CE/chi02/alpha20', 'CE/chi02/alpha50', 'CE/chi05/alpha02', 'CE/chi05/alpha05', 'CE/chi05/alpha10', 'CE/chi05/alpha20', 'CE/chi05/alpha50', 'CHE/chi00', 'CHE/chi01', 'CHE/chi02', 'CHE/chi05', 'GC/chi00', 'GC/chi01', 'GC/chi02', 'GC/chi05', 'NSC/chi00', 'NSC/chi01', 'NSC/chi02', 'NSC/chi05', 'SMT/chi00', 'SMT/chi01', 'SMT/chi02', 'SMT/chi05']\n"
     ]
    }
   ],
   "source": [
    "print(model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CE', 'CHE', 'GC', 'NSC', 'SMT']\n"
     ]
    }
   ],
   "source": [
    "print((list(kde_models.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chi00/alpha02',\n",
       " 'chi00/alpha05',\n",
       " 'chi00/alpha10',\n",
       " 'chi00/alpha20',\n",
       " 'chi00/alpha50',\n",
       " 'chi01/alpha02',\n",
       " 'chi01/alpha05',\n",
       " 'chi01/alpha10',\n",
       " 'chi01/alpha20',\n",
       " 'chi01/alpha50',\n",
       " 'chi02/alpha02',\n",
       " 'chi02/alpha05',\n",
       " 'chi02/alpha10',\n",
       " 'chi02/alpha20',\n",
       " 'chi02/alpha50',\n",
       " 'chi05/alpha02',\n",
       " 'chi05/alpha05',\n",
       " 'chi05/alpha10',\n",
       " 'chi05/alpha20',\n",
       " 'chi05/alpha50',\n",
       " 'chi00',\n",
       " 'chi01',\n",
       " 'chi02',\n",
       " 'chi05',\n",
       " 'chi00',\n",
       " 'chi01',\n",
       " 'chi02',\n",
       " 'chi05',\n",
       " 'chi00',\n",
       " 'chi01',\n",
       " 'chi02',\n",
       " 'chi05',\n",
       " 'chi00',\n",
       " 'chi01',\n",
       " 'chi02',\n",
       " 'chi05']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.split('/', 1)[1] for x in model_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Nhyper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m hyperparam_dict  \u001b[39m=\u001b[39m {}\n\u001b[1;32m      2\u001b[0m hyperidx\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mwhile\u001b[39;00m hyperidx \u001b[39m<\u001b[39m Nhyper:\n\u001b[1;32m      4\u001b[0m     hyperidx_with_Nhyper \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margwhere(np\u001b[39m.\u001b[39masarray([\u001b[39mlen\u001b[39m(x\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m)) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m hyperparams])\u001b[39m>\u001b[39mhyperidx)\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m      5\u001b[0m     hyperparams_at_level \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(\u001b[39mset\u001b[39m([x\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m)[hyperidx] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39masarray(hyperparams)[hyperidx_with_Nhyper]]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Nhyper' is not defined"
     ]
    }
   ],
   "source": [
    "hyperparam_dict  = {}\n",
    "hyperidx=0\n",
    "while hyperidx < Nhyper:\n",
    "    hyperidx_with_Nhyper = np.argwhere(np.asarray([len(x.split('/')) for x in hyperparams])>hyperidx).flatten()\n",
    "    hyperparams_at_level = sorted(set([x.split('/')[hyperidx] for x in np.asarray(hyperparams)[hyperidx_with_Nhyper]]))\n",
    "    hyperparam_dict[hyperidx] = hyperparams_at_level\n",
    "    hyperidx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models_at_deepest = all([len(x.split('/')[1:])==Nhyper for x in model_names])\n",
    "while all_models_at_deepest==False:\n",
    "    # loop until all models have the same length\n",
    "    for model in model_names:\n",
    "        # See number of hyperparameters in model, subtract one for channel\n",
    "        Nhyper_in_model = len(model.split('/'))-1\n",
    "        # loop until this model has all the hyperparam levels as well\n",
    "        while Nhyper_in_model < Nhyper:\n",
    "            model_names.remove(model)\n",
    "            for new_hyperparam in hyperparam_dict[Nhyper_in_model]:\n",
    "                # copy the same kde model for the higher hyperparam level\n",
    "                new_level = model.split('/') + [new_hyperparam]\n",
    "                # add new model name\n",
    "                model_names.append(model+'/'+new_hyperparam)\n",
    "            Nhyper_in_model += 1\n",
    "        model_names.sort()\n",
    "    # see if all models are at deepest level else repeat\n",
    "    all_models_at_deepest = all([len(x.split('/')[1:])==Nhyper for x in model_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chi05/alpha50', 'chi00/alpha20', 'chi00/alpha10', 'chi01/alpha02', 'chi05/alpha10', 'chi00/alpha05', 'chi02/alpha05', 'chi05/alpha02', 'chi00/alpha50', 'chi01/alpha50', 'chi01/alpha10', 'chi05/alpha05', 'chi01/alpha05', 'chi02/alpha02', 'chi00/alpha02', 'chi02/alpha50', 'chi01/alpha20', 'chi02/alpha20', 'chi05/alpha20', 'chi02/alpha10']\n",
      "0 2 {'chi01', 'chi00', 'chi02', 'chi05'}\n",
      "['chi05/alpha50', 'chi00/alpha20', 'chi00/alpha10', 'chi01/alpha02', 'chi05/alpha10', 'chi00/alpha05', 'chi02/alpha05', 'chi05/alpha02', 'chi00/alpha50', 'chi01/alpha50', 'chi01/alpha10', 'chi05/alpha05', 'chi01/alpha05', 'chi02/alpha02', 'chi00/alpha02', 'chi02/alpha50', 'chi01/alpha20', 'chi02/alpha20', 'chi05/alpha20', 'chi02/alpha10']\n",
      "1 2 {'alpha20', 'alpha10', 'alpha50', 'alpha05', 'alpha02'}\n"
     ]
    }
   ],
   "source": [
    "hyperparams = list(set([x.split('/', 1)[1] for x in model_names]))\n",
    "Nhyper = np.max([len(x.split('/')) for x in hyperparams])\n",
    "channels = sorted(list(set([x.split('/')[0] for x in model_names])))\n",
    "\n",
    "# construct dict that relates submodels to their index number\n",
    "submodels_dict = {} #dummy index dict keys:0,1,2,3, items: particular models\n",
    "ctr=0 #associates with either chi_b or alpha (0 or 1)\n",
    "while ctr < Nhyper:\n",
    "    print(hyperparams)\n",
    "    print(ctr, Nhyper, set([x.split('/')[ctr] for x in hyperparams]))\n",
    "    submodels_dict[ctr] = {}\n",
    "    hyper_set = sorted(list(set([x.split('/')[ctr] for x in hyperparams])))\n",
    "    for idx, model in enumerate(hyper_set): #idx associates with 0,1,2,3,(4) keys\n",
    "        submodels_dict[ctr][idx] = model\n",
    "    ctr += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {0: 'chi00', 1: 'chi01', 2: 'chi02', 3: 'chi05'}, 1: {0: 'alpha02', 1: 'alpha05', 2: 'alpha10', 3: 'alpha20', 4: 'alpha50'}}\n"
     ]
    }
   ],
   "source": [
    "print(submodels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[1.,3.01]\n",
    "model_list = []\n",
    "hyperparam_indices = []\n",
    "for hyper_idx in list(submodels_dict.keys()):\n",
    "    hyperparam_indices.append(int(np.floor(x[hyper_idx])))\n",
    "    model_list.append(submodels_dict[hyper_idx][int(np.floor(x[hyper_idx]))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3] ['chi01', 'alpha20']\n"
     ]
    }
   ],
   "source": [
    "print(hyperparam_indices, model_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Likelihood call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1 2. ]\n"
     ]
    }
   ],
   "source": [
    "label='SMT'\n",
    "cond_inputs =2\n",
    "hps = [[0.,0.1,0.2,0.5]]\n",
    "if label=='CE':\n",
    "    hps.append([0.2,0.5,1.,2.,5.])\n",
    "else:\n",
    "    hps.append([1])\n",
    "conditional_hp_idxs = [1,3]\n",
    "\n",
    "conditional_hps = []\n",
    "ctr=0 #associates with either chi_b or alpha (0 or 1)\n",
    "for i in range(cond_inputs):\n",
    "    conditional_hps.append(hps[i][conditional_hp_idxs[i]])\n",
    "\n",
    "conditional_hps = np.asarray(conditional_hps)\n",
    "print(conditional_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "data type 'Users/storm/CE_mappings.npy' not understood",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [56], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m filepath \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mUsers/storm/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      2\u001b[0m channel \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mCE\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mprint\u001b[39m(np\u001b[39m.\u001b[39;49mdtype(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mfilepath\u001b[39m}\u001b[39;49;00m\u001b[39m{\u001b[39;49;00mchannel\u001b[39m}\u001b[39;49;00m\u001b[39m_mappings.npy\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "\u001b[0;31mTypeError\u001b[0m: data type 'Users/storm/CE_mappings.npy' not understood"
     ]
    }
   ],
   "source": [
    "filepath = 'Users/storm/'\n",
    "channel = 'CE'\n",
    "print(np.dtype(f'{filepath}{channel}_mappings.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in GW data - hdf format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from populations import bbh_models, gw_obs\n",
    "params = ['mchirp', 'q','chieff','z']\n",
    "observations, obsdata, p_theta, events = gw_obs.generate_observations(params, '/Users/stormcolloms/Documents/PhD/Project_work/AMAZE_model_selection/gw_events', \\\n",
    "                                            100, 'delta', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46, 4)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(observations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46, 1, 4)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(obsdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.232875920000927 37.6363925625166 13.347090269689224 None\n",
      " 6.907754476512362 10.009991975305043]\n",
      "[8.24779881822069 38.20209787230986 14.019699977622743 None\n",
      " 6.907574735059984 10.008194723749112]\n",
      "[8.84889295963338 69.67672312100156 4.605170185988091 None\n",
      " 6.8932658118193295 9.86615]\n",
      "[9.121187841457722 91.48060423467516 9.049909624455028 None\n",
      " 6.482990596650801 6.549236468139789]\n",
      "[8.23212672756918 37.608213710983236 11.735077792414073 None\n",
      " 6.9072589617804185 10.005038059432552]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for channel in ['CE', 'CHE', 'GC', 'NSC', 'SMT']:\n",
    "    mappings = np.load(f'/Users/stormcolloms/Documents/PhD/Project_work/AMAZE_model_selection/flow_models/{channel}_mappings.npy', allow_pickle=True)\n",
    "    print(mappings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:11<00:00,  2.39s/it]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mwhile\u001b[39;00m ctr \u001b[39m<\u001b[39m Nhyper:\n\u001b[1;32m     44\u001b[0m     submodels_dict[ctr] \u001b[39m=\u001b[39m {}\n\u001b[0;32m---> 45\u001b[0m     hyper_set \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(\u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m([x\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m)[ctr] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m hyperparams])))\n\u001b[1;32m     46\u001b[0m     \u001b[39mfor\u001b[39;00m idx, model \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(hyper_set): \u001b[39m#idx associates with 0,1,2,3,(4) keys\u001b[39;00m\n\u001b[1;32m     47\u001b[0m         submodels_dict[ctr][idx] \u001b[39m=\u001b[39m model\n",
      "Cell \u001b[0;32mIn [5], line 45\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mwhile\u001b[39;00m ctr \u001b[39m<\u001b[39m Nhyper:\n\u001b[1;32m     44\u001b[0m     submodels_dict[ctr] \u001b[39m=\u001b[39m {}\n\u001b[0;32m---> 45\u001b[0m     hyper_set \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(\u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m([x\u001b[39m.\u001b[39;49msplit(\u001b[39m'\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m'\u001b[39;49m)[ctr] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m hyperparams])))\n\u001b[1;32m     46\u001b[0m     \u001b[39mfor\u001b[39;00m idx, model \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(hyper_set): \u001b[39m#idx associates with 0,1,2,3,(4) keys\u001b[39;00m\n\u001b[1;32m     47\u001b[0m         submodels_dict[ctr][idx] \u001b[39m=\u001b[39m model\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from populations import gw_obs, bbh_models\n",
    "x = np.random.rand(100,4)\n",
    "params = ['mchirp', 'q','chieff','z']\n",
    "channels = ['CE', 'CHE', 'GC', 'NSC', 'SMT']\n",
    "observations, obsdata, p_theta, events = gw_obs.generate_observations(params, '/Users/stormcolloms/Documents/PhD/Project_work/AMAZE_model_selection/gw_events', \\\n",
    "                                            100, 'posteriors', None)\n",
    "model_names, pop_models = bbh_models.get_models('/Users/stormcolloms/Documents/PhD/Project_work/OneChannel_Flows/models_reduced.hdf5', channels, params, use_flows=True,detectable=False)\n",
    "#TO CHANGE - add detectable models?\n",
    "model_names.sort()\n",
    "hyperparams = sorted(list(set([x.split('/', 1)[1] for x in model_names])))\n",
    "Nhyper = np.max([len(x.split('/')) for x in hyperparams])\n",
    "submodels_dict = {} #dummy index dict keys:0,1,2,3, items: particular models\n",
    "\n",
    "# create dict for the hyperparameters at each level\n",
    "hyperparam_dict  = {}\n",
    "hyperidx=0\n",
    "while hyperidx < Nhyper:\n",
    "    hyperidx_with_Nhyper = np.argwhere(np.asarray([len(x.split('/')) for x in hyperparams])>hyperidx).flatten()\n",
    "    hyperparams_at_level = sorted(set([x.split('/')[hyperidx] for x in np.asarray(hyperparams)[hyperidx_with_Nhyper]]))\n",
    "    hyperparam_dict[hyperidx] = hyperparams_at_level\n",
    "    hyperidx += 1\n",
    "    \n",
    "all_models_at_deepest = all([len(x.split('/')[1:])==Nhyper for x in model_names])\n",
    "while all_models_at_deepest==False:\n",
    "    # loop until all models have the same length\n",
    "    for model in model_names:\n",
    "        # See number of hyperparameters in model, subtract one for channel\n",
    "        Nhyper_in_model = len(model.split('/'))-1\n",
    "        # loop until this model has all the hyperparam levels as well\n",
    "        while Nhyper_in_model < Nhyper:\n",
    "            model_names.remove(model)\n",
    "            for new_hyperparam in hyperparam_dict[Nhyper_in_model]:\n",
    "                # add new model name\n",
    "                model_names.append(model+'/'+new_hyperparam)\n",
    "            Nhyper_in_model += 1\n",
    "        model_names.sort()\n",
    "    # see if all models are at deepest level else repeat\n",
    "    all_models_at_deepest = all([len(x.split('/')[1:])==Nhyper for x in model_names])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr=0 #associates with either chi_b or alpha (0 or 1)\n",
    "while ctr < Nhyper:\n",
    "    submodels_dict[ctr] = {}\n",
    "    hyper_set = sorted(list(set([x.split('/')[ctr] for x in hyperparams])))\n",
    "    for idx, model in enumerate(hyper_set): #idx associates with 0,1,2,3,(4) keys\n",
    "        submodels_dict[ctr][idx] = model\n",
    "    ctr += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping all observations at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from populations.Flowsclass_dev import FlowModel\n",
    "from populations import gw_obs\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "popsynth_outputs = read_hdf5(file_path, 'CE')\n",
    "flow = FlowModel.from_samples('CE', popsynth_outputs, params)\n",
    "\n",
    "gw_path = '/Users/stormcolloms/Documents/PhD/Project_work/AMAZE_model_selection/gw_events'\n",
    "observations, obsdata, p_theta, events = gw_obs.generate_observations(params, gw_path, \\\n",
    "                                            100, 'posteriors', None)\n",
    "flow.load_model('/Users/stormcolloms/Documents/PhD/Project_work/AMAZE_model_selection/flow_models/', 'CE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappedobs = np.asarray(flow.map_obs(obsdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46, 100, 2)\n"
     ]
    }
   ],
   "source": [
    "conditional_hps = [0.3,1]\n",
    "tileconds = np.repeat([conditional_hps],np.shape(mappedobs)[1], axis=0)\n",
    "tileconds = np.repeat([tileconds],np.shape(mappedobs)[0], axis=0)\n",
    "print(np.shape(tileconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = torch.from_numpy(mappedobs.astype(np.float32))\n",
    "hyperparams = torch.from_numpy(tileconds.astype(np.float32))\n",
    "shape = sample.shape\n",
    "#flatten samples given they are have dimensions Nsampels x Nobs x Nparams\n",
    "sample = torch.flatten(sample, start_dim=0, end_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = torch.reshape(sample, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([46, 100, 4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('amaze')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40f4eedd3c2b0a4cf443053300595e4c410b663f293e84798d1742802fb2c7c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
